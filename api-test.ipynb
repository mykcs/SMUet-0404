{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 1708009,
     "sourceType": "datasetVersion",
     "datasetId": 958228
    },
    {
     "sourceId": 7972942,
     "sourceType": "datasetVersion",
     "datasetId": 4691017,
     "isSourceIdPinned": false
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "!pip install lightning\n",
    "!pip install wandb -U\n",
    "!pip install monai"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T14:19:15.428619Z",
     "iopub.execute_input": "2024-04-04T14:19:15.428962Z",
     "iopub.status.idle": "2024-04-04T14:20:04.349489Z",
     "shell.execute_reply.started": "2024-04-04T14:19:15.428932Z",
     "shell.execute_reply": "2024-04-04T14:20:04.348132Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:58:19.378665Z",
     "start_time": "2024-04-04T18:58:12.228490Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: segmentation-models-pytorch in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (0.3.3)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.14.1)\r\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.4)\r\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.1)\r\n",
      "Requirement already satisfied: timm==0.9.2 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.9.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (4.66.2)\r\n",
      "Requirement already satisfied: pillow in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from segmentation-models-pytorch) (10.2.0)\r\n",
      "Requirement already satisfied: torch in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\r\n",
      "Requirement already satisfied: munch in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (4.9.0)\r\n",
      "Requirement already satisfied: numpy in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\r\n",
      "Requirement already satisfied: requests in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: filelock in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.13.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2024.2.2)\r\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: lightning in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (6.0.1)\r\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.3.1)\r\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (0.11.2)\r\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (1.24.3)\r\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (23.2)\r\n",
      "Requirement already satisfied: torch<4.0,>=1.13.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (1.13.1)\r\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (1.3.2)\r\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (4.9.0)\r\n",
      "Requirement already satisfied: pytorch-lightning in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning) (2.2.1)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (68.2.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\r\n",
      "Requirement already satisfied: idna>=2.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.4)\r\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: wandb in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (0.16.6)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (5.9.0)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (1.44.0)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: PyYAML in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: setproctitle in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (68.2.2)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (1.4.4)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (4.9.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from wandb) (4.25.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: monai in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from monai) (1.24.3)\r\n",
      "Requirement already satisfied: torch>=1.9 in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from monai) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages (from torch>=1.9->monai) (4.9.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# 导入的库\n",
    "import IPython\n",
    "import monai\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from IPython.display import display\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "from pytorch_lightning.core.mixins import HyperparametersMixin\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "print(\"Envirionment Set Up.\")"
   ],
   "metadata": {
    "_uuid": "36007df4-0fc3-4938-9a09-f54d22d06e5f",
    "_cell_guid": "c62342c1-1564-439b-b5f2-8bc2c518bdaf",
    "execution": {
     "iopub.status.busy": "2024-04-04T14:30:19.305178Z",
     "iopub.execute_input": "2024-04-04T14:30:19.306065Z",
     "iopub.status.idle": "2024-04-04T14:30:19.319632Z",
     "shell.execute_reply.started": "2024-04-04T14:30:19.306029Z",
     "shell.execute_reply": "2024-04-04T14:30:19.318688Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envirionment Set Up.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset and Augment Setting"
   ],
   "metadata": {
    "_uuid": "5cf48cd2-93fe-4aac-ada9-1a19cd8b2e56",
    "_cell_guid": "0134c009-4039-4716-893f-56425743cab6",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import albumentations as A\n",
    "\n",
    "# 定义数据增强\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(),\n",
    "    A.ElasticTransform(p=0.3, alpha=120, sigma=120 * 0.08, alpha_affine=120 * 0.015),\n",
    "    A.RandomSizedCrop(min_max_height=(128, 256), height=256, width=256, p=0.3),\n",
    "    # A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    # A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2(),\n",
    "])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T14:30:22.903909Z",
     "iopub.execute_input": "2024-04-04T14:30:22.904641Z",
     "iopub.status.idle": "2024-04-04T14:30:22.914871Z",
     "shell.execute_reply.started": "2024-04-04T14:30:22.904604Z",
     "shell.execute_reply": "2024-04-04T14:30:22.913759Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:58:23.376805Z",
     "start_time": "2024-04-04T18:58:23.374095Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/lwj6dcgj7vgbzkyrpwn082pr0000gn/T/ipykernel_65690/2065298405.py:10: DeprecationWarning: Initializing with 'height' and 'width' is deprecated. Please use a tuple (height, width) for the 'size' argument.\n",
      "  A.RandomSizedCrop(min_max_height=(128, 256), height=256, width=256, p=0.3),\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def adjust_window(image, window_center, window_width):\n",
    "    \"\"\"\n",
    "    调整CT图像的窗宽窗位。\n",
    "    :param image: 输入的图像数组。\n",
    "    :param window_center: 窗位（WC）。\n",
    "    :param window_width: 窗宽（WW）。\n",
    "    :return: 调整窗宽窗位后的图像。\n",
    "    \"\"\"\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    windowed_img = np.clip(image, img_min, img_max)\n",
    "    # print(windowed_img.dtype) # NOW its float64\n",
    "    return windowed_img\n",
    "\n",
    "\n",
    "class MultipleImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, transform=None):\n",
    "        \"\"\"\n",
    "        image_paths: 图像文件路径列表\n",
    "        label_paths: 标签文件路径列表\n",
    "        transform: 应用于图像和标签的转换操作\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # 假设图像和标签列表长度相等\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):  # dataloader获取每个数据都会用到这个函数，所以你应当在这里实现你需要的\n",
    "        # 预处理等等步骤\n",
    "        image = (np.load(self.image_paths[idx]))['arr_0']\n",
    "        #         print(image.shape)\n",
    "        label = (np.load(self.label_paths[idx]))['arr_0']\n",
    "        #         print(label.shape)\n",
    "\n",
    "        image = adjust_window(image, window_center=40, window_width=400)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=label)\n",
    "            image = augmented['image']\n",
    "            #             print(\"image aug\")\n",
    "            label = augmented['mask']\n",
    "            image = image.float()\n",
    "            label = label.long()\n",
    "\n",
    "        label = label.long()\n",
    "        # normalize image here based on its value accordingly\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        # print(image.unique())\n",
    "        #         print(f\"label shape: {label.shape}\")\n",
    "        #         print(image.shape)\n",
    "        return image.float(), label.long()\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "class MOADataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_transform = train_transform\n",
    "        self.val_transform = val_transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        image_dir = os.path.join(self.data_dir, 'image_npz')  # 注意这里路径的更正\n",
    "        label_dir = os.path.join(self.data_dir, 'mask_npz')\n",
    "\n",
    "        # 读取文件路径\n",
    "        image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.npz')])\n",
    "        label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.npz')])\n",
    "\n",
    "        # 划分训练集、验证集、测试集\n",
    "        train_size = int(0.8 * len(image_files))\n",
    "        val_size = int(0.1 * len(image_files))\n",
    "\n",
    "        self.train_image_paths = image_files[:train_size]\n",
    "        self.val_image_paths = image_files[train_size:train_size + val_size]\n",
    "        self.test_image_paths = image_files[train_size + val_size:]\n",
    "\n",
    "        self.train_label_paths = label_files[:train_size]\n",
    "        self.val_label_paths = label_files[train_size:train_size + val_size]\n",
    "        self.test_label_paths = label_files[train_size + val_size:]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = MultipleImageDataset(self.train_image_paths, self.train_label_paths,\n",
    "                                             transform=self.train_transform)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = MultipleImageDataset(self.val_image_paths, self.val_label_paths, transform=self.val_transform)\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = MultipleImageDataset(self.test_image_paths, self.test_label_paths, transform=self.val_transform)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n"
   ],
   "metadata": {
    "_uuid": "d3ebdfad-a463-469d-812a-9214d9338d08",
    "_cell_guid": "cfd87ac0-9db7-4743-9831-9f75542fc41c",
    "collapsed": false,
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:13.830143Z",
     "iopub.execute_input": "2024-04-04T14:31:13.830522Z",
     "iopub.status.idle": "2024-04-04T14:31:13.867404Z",
     "shell.execute_reply.started": "2024-04-04T14:31:13.830484Z",
     "shell.execute_reply": "2024-04-04T14:31:13.866350Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:58:23.384271Z",
     "start_time": "2024-04-04T18:58:23.377704Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义数据集和数据加载器\n",
    "# data_dir = '/kaggle/input/brats2018'\n",
    "data_dir = 'kaggle/input/rawniidataset/SMU_Dataset'\n",
    "\n",
    "\n",
    "def predict_and_log_images(num_samples=2):\n",
    "    # 假设 test_loader 和 model 已经定义好了，并且 model 已经移动到了适当的设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    test_loader = data_module.test_dataloader()\n",
    "\n",
    "    # 生成随机索引\n",
    "    indices = torch.randperm(len(test_loader.dataset))[:num_samples]\n",
    "    # 调整subplot的大小\n",
    "    fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))  # 每个样本显示3张图（原图、真实掩码、预测掩码）\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = test_loader.dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)  # 添加batch维度并移动到设备\n",
    "        mask = mask.squeeze()  # 移除batch维度（如果有的话）\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(image)\n",
    "            prediction = torch.argmax(pred, dim=1).cpu()  # 获取预测类别并移回CPU\n",
    "\n",
    "        # 显示原始图像\n",
    "        axs[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
    "        axs[i, 0].set_title(f'Original Image {i + 1}')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # 显示Ground Truth\n",
    "        axs[i, 1].imshow(mask.cpu().numpy(), cmap='gray')\n",
    "        axs[i, 1].set_title(f'True Mask {i + 1}')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        # 显示预测掩码\n",
    "        axs[i, 2].imshow(prediction[0].numpy(), cmap='gray')\n",
    "        axs[i, 2].set_title(f'Predicted Mask {i + 1}')\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)  # 防止在notebook中显示图像\n",
    "    return fig\n",
    "\n",
    "\n",
    "# 自定义回调，用于在验证结束时执行操作\n",
    "class ValidationCallback(Callback):\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        print(\"Validation epoch ended. Executing custom actions...\")\n",
    "        # 假设test_loader已经在pl_module（即您的model）中定义\n",
    "        # 或者您可以在这里直接访问datamodule的test_dataloader\n",
    "        #         test_loader = pl_module.test_dataloader()\n",
    "\n",
    "        # 确保模型处于评估模式\n",
    "        #         pl_module.eval()\n",
    "        #         print(\"eval activated\")\n",
    "\n",
    "        # 执行自定义的预测和绘图逻辑\n",
    "        fig = predict_and_log_images(num_samples=2)\n",
    "        # 用wandb记录图像，或进行其他操作\n",
    "        wandb.log({\"Validation Callback Predicted Images\": wandb.Image(fig)})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:58:58.179936Z",
     "start_time": "2024-04-04T18:58:58.174969Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (Opt) Data Pre-Check"
   ],
   "metadata": {
    "_uuid": "877d1009-7e88-4dff-8d32-34f278b9ba2f",
    "_cell_guid": "f5aba93a-1cc3-4f13-9763-b1bbe5b31453",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始化数据模块\n",
    "data_module = MOADataModule(data_dir='kaggle/input/rawniidataset/SMU_Dataset', batch_size=16)\n",
    "# data_module = MOADataModule(data_dir='/kaggle/input/brats2018', batch_size=16)\n",
    "# 设置数据模块（准备数据）\n",
    "data_module.setup()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:02.277146Z",
     "start_time": "2024-04-04T18:59:02.270856Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# 获取训练数据加载器\n",
    "train_loader = data_module.train_dataloader()\n",
    "\n",
    "# 从数据加载器中抽取一批数据\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# 选择要展示的图像数量\n",
    "num_images_to_show = 4\n",
    "\n",
    "# 创建图表来展示图像和对应的掩码\n",
    "fig, axs = plt.subplots(num_images_to_show, 3, figsize=(15, num_images_to_show * 5))\n",
    "\n",
    "for i in range(num_images_to_show):\n",
    "    img = images[i].squeeze().numpy()  # 假设图像和掩码都只有一个通道\n",
    "    lbl = labels[i].squeeze().numpy()\n",
    "    overlay = np.ma.masked_where(lbl == 0, lbl)\n",
    "\n",
    "    axs[i, 0].imshow(img, cmap='gray')\n",
    "    axs[i, 0].set_title('Image')\n",
    "    axs[i, 0].axis('off')\n",
    "\n",
    "    axs[i, 1].imshow(lbl, cmap='gray')\n",
    "    axs[i, 1].set_title('Mask')\n",
    "    axs[i, 1].axis('off')\n",
    "\n",
    "    axs[i, 2].imshow(img, cmap='gray')\n",
    "    axs[i, 2].imshow(overlay, cmap='autumn', alpha=0.5)\n",
    "    axs[i, 2].set_title('Overlay')\n",
    "    axs[i, 2].axis('off')\n",
    "\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Label shape: {lbl.shape}\")\n",
    "    print(f\"Image M&m Value: {img.max(), img.min()}\")\n",
    "    print(f\"Label Unique: {np.unique(lbl)}\")\n"
   ],
   "metadata": {
    "_uuid": "5ed77956-2d29-4ad9-a5d9-5dde6b33a2e9",
    "_cell_guid": "21a0fb7b-2f3c-446b-96cf-4e998b9970d8",
    "_kg_hide-input": false,
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:20.820069Z",
     "iopub.execute_input": "2024-04-04T14:31:20.820445Z",
     "iopub.status.idle": "2024-04-04T14:31:24.917845Z",
     "shell.execute_reply.started": "2024-04-04T14:31:20.820416Z",
     "shell.execute_reply": "2024-04-04T14:31:24.916931Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:04.574237Z",
     "start_time": "2024-04-04T18:59:03.425070Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 65748) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/queues.py:107\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    106\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/selectors.py:415\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 415\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 65748) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m data_module\u001B[38;5;241m.\u001B[39mtrain_dataloader()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 从数据加载器中抽取一批数据\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# 选择要展示的图像数量\u001B[39;00m\n\u001B[1;32m      8\u001B[0m num_images_to_show \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 65748) exited unexpectedly"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.tight_layout()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:05.436682Z",
     "start_time": "2024-04-04T18:59:05.421121Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show Time!\n"
     ]
    }
   ],
   "source": [
    "plt.show()\n",
    "print('Show Time!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:08.603592Z",
     "start_time": "2024-04-04T18:59:08.601518Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Setting"
   ],
   "metadata": {
    "_uuid": "612e9ff5-ea09-4a38-bc45-d077638c795f",
    "_cell_guid": "13f85698-ebef-40de-a063-58b836eb7980",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        #         print(f\"x1 Shape is: {x1.shape}\")\n",
    "        #         print(f\"x2 Shape is: {x2.shape}\")\n",
    "        #         print(f\"x Shape is: {x.shape}\")\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=1, n_classes=14, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         print(f\"Input shape: {x.shape}\")\n",
    "        x1 = self.inc(x)\n",
    "        #         print(f\"After inc (DoubleConv): {x1.shape}\")\n",
    "        x2 = self.down1(x1)\n",
    "        #         print(f\"After down1: {x2.shape}\")\n",
    "        x3 = self.down2(x2)\n",
    "        #         print(f\"After down2: {x3.shape}\")\n",
    "        x4 = self.down3(x3)\n",
    "        #         print(f\"After down3: {x4.shape}\")\n",
    "        x5 = self.down4(x4)\n",
    "        #         print(f\"After down4: {x5.shape}\")\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        #         print(f\"After up1 (with skip connection from down3): {x.shape}\")\n",
    "        x = self.up2(x, x3)\n",
    "        #         print(f\"After up2 (with skip connection from down2): {x.shape}\")\n",
    "        x = self.up3(x, x2)\n",
    "        #         print(f\"After up3 (with skip connection from down1): {x.shape}\")\n",
    "        x = self.up4(x, x1)\n",
    "        #         print(f\"After up4 (with skip connection from inc): {x.shape}\")\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        #         print(f\"Output shape (after OutConv): {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:28.993760Z",
     "iopub.execute_input": "2024-04-04T14:31:28.994167Z",
     "iopub.status.idle": "2024-04-04T14:31:29.024473Z",
     "shell.execute_reply.started": "2024-04-04T14:31:28.994130Z",
     "shell.execute_reply": "2024-04-04T14:31:29.023255Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:11.308978Z",
     "start_time": "2024-04-04T18:59:11.299685Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 损失函数： \n",
    "你使用了monai.losses.DiceCELoss ，一个结合了Dice Loss和 Cross-Entropy Loss 的混合损失函数。这种混合能帮助模型更稳定地收敛，并平衡精确性和召回率。\n",
    "对于医学图像分割任务，这是一个合理的选择。\n",
    "除了 Dice Loss 外，医学图像中常使用的分割损失函数还有 Focal Loss，Tversky Loss，Generalized Dice Loss 等等。您可以进行实验，看看不同的损失函数是否会对模型的性能产生影响。\n",
    "\n",
    "评价指标： \n",
    "你使用了 torchmetrics 库计算精度、微观精度、宏观精度、Dice系数、F1 score、Jaccard系数等指标。这些指标能全面评估分割模型的性能。\n",
    "\n",
    "优化器和学习率调度器： \n",
    "使用了 Adam 优化器和 CosineAnnealingLR 学习率调度器。Adam 是深度学习中常用的自适应优化器，余弦退火学习率能有效平稳地降低学习率并有助于模型收敛。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from monai.transforms import AsDiscrete\n",
    "class UNetTestModel(pl.LightningModule, HyperparametersMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_name='resnet50',\n",
    "            encoder_weights='imagenet',\n",
    "            in_channels=1,\n",
    "            classes=14,\n",
    "            #         loss_fn=monai.losses.FocalLoss(use_softmax=True, to_onehot_y=True, include_background=False),\n",
    "            loss_fn=monai.losses.DiceCELoss(softmax=True, lambda_dice=0.85, lambda_ce=0.15, to_onehot_y=True),\n",
    "            #         loss_fn=monai.losses.DiceLoss( to_onehot_y=True),\n",
    "            loss_function='DiceCELoss',\n",
    "            learning_rate=5e-3,\n",
    "            #         train_accuracy_params={'task': \"multiclass\", 'num_classes': 14, 'average': 'macro'},\n",
    "            #         val_accuracy_params={'task': \"multiclass\", 'num_classes': 14, 'average': 'macro'},\n",
    "            #         dice_params={'multiclass': True, 'num_classes': 14, 'average': 'macro'},  \n",
    "            bilinear=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        ###################### model #########################\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            #             decoder_attention_type='scse',\n",
    "        )\n",
    "        #         self.model = UNet(in_channels, classes, bilinear=bilinear)\n",
    "        ###################### loss ##########################\n",
    "        self.loss_fn = loss_fn\n",
    "        ###################### metrics ######################\n",
    "        #         self.train_accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes, average='micro', ignore_index=0)\n",
    "        self.val_accuracy_MACRO = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes,\n",
    "                                                                       average='macro', ignore_index=0)\n",
    "        self.val_accuracy_micro = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes,\n",
    "                                                                       average='micro', ignore_index=0)\n",
    "        self.Dice = torchmetrics.classification.Dice(multiclass=True, num_classes=classes, average='macro',\n",
    "                                                     ignore_index=0)\n",
    "        self.F1 = torchmetrics.classification.MulticlassF1Score(num_classes=classes, average=\"macro\", ignore_index=0)\n",
    "        self.Jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=classes, average=\"macro\",\n",
    "                                                                          ignore_index=0)\n",
    "        #####################################################\n",
    "\n",
    "    #         Outputs Shape: torch.Size([1, 14, 256, 256])\n",
    "    #         Labels Shape: torch.Size([1, 256, 256])\n",
    "    #         Labels Shape After Unsqueeze: torch.Size([1, 1, 256, 256])\n",
    "    #         Predictions Shape: torch.Size([1, 256, 256])\n",
    "\n",
    "    # 定义前向传播\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # 定义单个训练步的计算流程\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.forward(images)\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels.unsqueeze(1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "\n",
    "        \"\"\"\n",
    "        # test01没有\n",
    "        这两行代码是用来计算精度的，Dice系数。在某些情况下，您可能想要在 训练过程 中监控这些指标。但是需要注意:\n",
    "        计算开销: 在每个训练步上计算精度这些指标会带来一定的计算量开销，有可能减慢训练速度。\n",
    "        训练目标: 最主要的训练目标是让模型学习如何最小化损失函数，因此损失是必须返回的内容。指标的监控可以放在 验证 步骤中，或每个 epoch 结束时再集中计算。\n",
    "        \n",
    "        acc = self.train_accuracy(outputs, labels)\n",
    "        self.log('train_accuracy', acc, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        \n",
    "        Dice = self.Dice(outputs, labels)\n",
    "        self.log('train_Dice', Dice, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        \"\"\"\n",
    "        return loss\n",
    "\n",
    "    # 定义单个验证步的计算流程\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #         print(\"val_step going\")\n",
    "        images, labels = batch\n",
    "        outputs = self.forward(images)\n",
    "        loss = self.loss_fn(outputs, labels.unsqueeze(1))\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        #         print(labels_one_hot.shape)\n",
    "        acc_micro = self.val_accuracy_micro(preds, labels)\n",
    "        acc_MACRO = self.val_accuracy_MACRO(preds, labels)\n",
    "        Dice = self.Dice(preds, labels)\n",
    "        F1 = self.F1(preds, labels)\n",
    "        Jaccard = self.Jaccard(preds, labels)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        #         for i,acc in enumerate(acc):\n",
    "        #             self.log(f'val_accuracy_MACRO_{i}', acc, on_step=True, on_epoch=False,logger=True, prog_bar=True)\n",
    "        #         for i,F1 in enumerate(F1):\n",
    "        #             self.log(f'val_F1_{i}', F1, on_step=True, on_epoch=False,logger=True, prog_bar=True)\n",
    "        self.log('val_accuracy_micro', acc_micro, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        self.log('val_accuracy_MACRO', acc_MACRO, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        self.log('val_F1', F1, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        self.log('val_Dice', Dice, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "        self.log('val_Jaccard', Jaccard, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
    "\n",
    "    #     def test_step(self, batch, batch_idx):\n",
    "    #         images, labels = batch\n",
    "    #         outputs = self.forward(images)\n",
    "    #         loss = self.loss_fn(outputs, labels.unsqueeze(1))\n",
    "\n",
    "    #         preds = torch.argmax(outputs, dim=1)\n",
    "    #         acc_MACRO = self.val_accuracy_MACRO(preds, labels)  # 注意: 在测试阶段，我们仍然可以使用val_accuracy_MACRO\n",
    "    #         Dice = self.Dice(preds, labels)\n",
    "    #         self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    #         self.log('test_accuracy', acc_MACRO, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    #         self.log('test_Dice', Dice, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    #         # 这里没有wandb.log调用，因为PyTorch Lightning的self.log已经足够记录所有指标\n",
    "\n",
    "    # 定义优化器更新参数的逻辑，以及其他潜在的自定义操作，如学习率暖身\n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure, **kwargs):\n",
    "        # 调用优化器的step方法前执行自定义操作\n",
    "        # 比如实现学习率热启动\n",
    "        if self.trainer.global_step < 25:\n",
    "            lr_scale = min(1.0, float(self.trainer.global_step + 1) / 25)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = lr_scale * self.hparams.learning_rate\n",
    "        # 调用优化器的step方法来更新模型参数\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "\n",
    "    # 配置优化器和学习率调度器的策略\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9, verbose=True)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.000001, last_epoch=-1)\n",
    "        #         print(self.hparams.learning_rate)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',  # 指定更新学习率的间隔单位为'epoch'\n",
    "                'frequency': 1,  # 每个epoch更新一次学习率\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "_uuid": "ac36333a-0d4f-494b-a95c-889fc8ca64d9",
    "_cell_guid": "e855577b-fc8c-4409-b4d6-0f82ee37f1d3",
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:35.746373Z",
     "iopub.execute_input": "2024-04-04T14:31:35.746738Z",
     "iopub.status.idle": "2024-04-04T14:31:35.772538Z",
     "shell.execute_reply.started": "2024-04-04T14:31:35.746710Z",
     "shell.execute_reply": "2024-04-04T14:31:35.771673Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:14.730622Z",
     "start_time": "2024-04-04T18:59:14.715784Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (Opt) Hyper Params Check"
   ],
   "metadata": {
    "_uuid": "7357d769-1c7b-4ae0-b717-68fe4d9524bc",
    "_cell_guid": "f00537a2-4837-4a41-94fe-819c9ce7b959",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Collecting torchinfo\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl (23 kB)\r\n",
      "Installing collected packages: torchinfo\r\n",
      "Successfully installed torchinfo-1.8.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install torchinfo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:36.462231Z",
     "start_time": "2024-04-04T18:59:34.123051Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = UNetTestModel()\n",
    "print(summary(model, input_size=(1, 1, 256, 256)))\n"
   ],
   "metadata": {
    "_uuid": "54a3656f-6ee2-41bb-92cc-2f278e3ed7d7",
    "_cell_guid": "e6d612dd-14b3-4d64-960e-78b581b1d7e3",
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:40.051168Z",
     "iopub.execute_input": "2024-04-04T14:31:40.051528Z",
     "iopub.status.idle": "2024-04-04T14:31:41.031927Z",
     "shell.execute_reply.started": "2024-04-04T14:31:40.051499Z",
     "shell.execute_reply": "2024-04-04T14:31:41.030542Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:45.794020Z",
     "start_time": "2024-04-04T18:59:39.325131Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'loss_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_fn'])`.\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /Users/myk/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0.00/97.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "216d234950bd4409ba0d77ee602204ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "UNetTestModel                                      [1, 14, 256, 256]         --\n",
      "├─Unet: 1-1                                        [1, 14, 256, 256]         --\n",
      "│    └─ResNetEncoder: 2-1                          [1, 1, 256, 256]          --\n",
      "│    │    └─Conv2d: 3-1                            [1, 64, 128, 128]         3,136\n",
      "│    │    └─BatchNorm2d: 3-2                       [1, 64, 128, 128]         128\n",
      "│    │    └─ReLU: 3-3                              [1, 64, 128, 128]         --\n",
      "│    │    └─MaxPool2d: 3-4                         [1, 64, 64, 64]           --\n",
      "│    │    └─Sequential: 3-5                        [1, 256, 64, 64]          215,808\n",
      "│    │    └─Sequential: 3-6                        [1, 512, 32, 32]          1,219,584\n",
      "│    │    └─Sequential: 3-7                        [1, 1024, 16, 16]         7,098,368\n",
      "│    │    └─Sequential: 3-8                        [1, 2048, 8, 8]           14,964,736\n",
      "│    └─UnetDecoder: 2-2                            [1, 16, 256, 256]         --\n",
      "│    │    └─Identity: 3-9                          [1, 2048, 8, 8]           --\n",
      "│    │    └─ModuleList: 3-10                       --                        9,012,928\n",
      "│    └─SegmentationHead: 2-3                       [1, 14, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-11                           [1, 14, 256, 256]         2,030\n",
      "│    │    └─Identity: 3-12                         [1, 14, 256, 256]         --\n",
      "│    │    └─Activation: 3-13                       [1, 14, 256, 256]         --\n",
      "====================================================================================================\n",
      "Total params: 32,516,718\n",
      "Trainable params: 32,516,718\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 10.65\n",
      "====================================================================================================\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 304.61\n",
      "Params size (MB): 130.07\n",
      "Estimated Total Size (MB): 434.94\n",
      "====================================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bilinear\":        False\n",
      "\"classes\":         14\n",
      "\"encoder_name\":    resnet50\n",
      "\"encoder_weights\": imagenet\n",
      "\"in_channels\":     1\n",
      "\"learning_rate\":   0.005\n",
      "\"loss_fn\":         DiceCELoss(\n",
      "  (dice): DiceLoss()\n",
      "  (cross_entropy): CrossEntropyLoss()\n",
      "  (binary_cross_entropy): BCEWithLogitsLoss()\n",
      ")\n",
      "\"loss_function\":   DiceCELoss\n"
     ]
    }
   ],
   "source": [
    "print(model.hparams)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:46.818445Z",
     "start_time": "2024-04-04T18:59:46.816489Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (Opt) Pre-Train Test"
   ],
   "metadata": {
    "_uuid": "7e5a9389-57b6-4a07-b428-b41fcc987525",
    "_cell_guid": "de47c7ee-1eee-4ee1-93f6-d6b43df86981",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_module = MOADataModule(data_dir=data_dir, batch_size=1)\n",
    "# avail test\n",
    "model = UNetTestModel()\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "############################################## fastrun ###################################################\n",
    "trainer = pl.Trainer(max_epochs=20,\n",
    "                     fast_dev_run=True,\n",
    "                     #                      callbacks=[lr_monitor, ValidationCallback()],\n",
    "                     #                      check_val_every_n_epoch=10, \n",
    "                     )\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "_uuid": "3c026544-ec35-408c-9c4a-8840ba0d8202",
    "_cell_guid": "37e59d03-78db-479e-954e-7e8fb28a80a3",
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:43.970528Z",
     "iopub.execute_input": "2024-04-04T14:31:43.971274Z",
     "iopub.status.idle": "2024-04-04T14:31:47.673257Z",
     "shell.execute_reply.started": "2024-04-04T14:31:43.971243Z",
     "shell.execute_reply": "2024-04-04T14:31:47.672147Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T18:59:51.788581Z",
     "start_time": "2024-04-04T18:59:49.165715Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name               | Type                   | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model              | Unet                   | 32.5 M\n",
      "1 | loss_fn            | DiceCELoss             | 0     \n",
      "2 | val_accuracy_MACRO | MulticlassAccuracy     | 0     \n",
      "3 | val_accuracy_micro | MulticlassAccuracy     | 0     \n",
      "4 | Dice               | Dice                   | 0     \n",
      "5 | F1                 | MulticlassF1Score      | 0     \n",
      "6 | Jaccard            | MulticlassJaccardIndex | 0     \n",
      "--------------------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.067   Total estimated model params size (MB)\n",
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21995ee42c9d4afd96ea86747ce5b6ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 65803) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/queues.py:107\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    106\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/selectors.py:415\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 415\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 65803) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 11\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m############################################## fastrun ###################################################\u001B[39;00m\n\u001B[1;32m      6\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,\n\u001B[1;32m      7\u001B[0m                      fast_dev_run\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      8\u001B[0m                      \u001B[38;5;66;03m#                      callbacks=[lr_monitor, ValidationCallback()],\u001B[39;00m\n\u001B[1;32m      9\u001B[0m                      \u001B[38;5;66;03m#                      check_val_every_n_epoch=10, \u001B[39;00m\n\u001B[1;32m     10\u001B[0m                      )\n\u001B[0;32m---> 11\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1031\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m     batch, _, __ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     batch_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:133\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatches\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:60\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_profiler()\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _ITERATOR_RETURN:\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n\u001B[1;32m    343\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001B[0m, in \u001B[0;36m_MaxSizeCycle.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n):\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 78\u001B[0m         out[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterators\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consumed[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 65803) exited unexpectedly"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_module = MOADataModule(data_dir=data_dir, batch_size=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T19:00:10.556719Z",
     "start_time": "2024-04-04T19:00:10.554778Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# learnability test\n",
    "wandb_logger_test = WandbLogger()\n",
    "# # 初始化Trainer，设置overfit_batches来过拟合一小部分数据\n",
    "# lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "test_trainer = pl.Trainer(\n",
    "    overfit_batches=1,\n",
    "    logger=wandb_logger_test,\n",
    "    #                           callbacks=[lr_monitor, ValidationCallback()], \n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "# # 或者，使用10个批次的数据来过拟合\n",
    "# test_trainer = pl.Trainer(overfit_batches=10, logger=wandb_logger_test)\n"
   ],
   "metadata": {
    "_uuid": "273bd454-c583-441a-8b86-d5b109f13028",
    "_cell_guid": "4417d2cf-aade-490c-9cf2-4aed7f9bb6f0",
    "execution": {
     "iopub.status.busy": "2024-04-04T14:31:50.668991Z",
     "iopub.execute_input": "2024-04-04T14:31:50.669902Z",
     "iopub.status.idle": "2024-04-04T15:03:03.330392Z",
     "shell.execute_reply.started": "2024-04-04T14:31:50.669863Z",
     "shell.execute_reply": "2024-04-04T15:03:03.329580Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T19:00:16.046129Z",
     "start_time": "2024-04-04T19:00:16.010410Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mkagura0108\u001B[0m (\u001B[33mmky18\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/myk/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='17acd1bc31a78695a83ccc49a28bee44d314e55e')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T19:06:23.002312Z",
     "start_time": "2024-04-04T19:06:21.103371Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (961506394.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[21], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    api = wandb.Api(17acd1bc31a78695a83ccc49a28bee44d314e55e)\u001B[0m\n\u001B[0m                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T19:01:23.016587Z",
     "start_time": "2024-04-04T19:01:23.012764Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmykcs\u001B[0m (\u001B[33mteam-mykcs\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01113681017777771, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "559fb3eab4314d218e38a237487d34be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>./wandb/run-20240405_030633-rdomjq13</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/team-mykcs/lightning_logs/runs/rdomjq13' target=\"_blank\">atomic-bee-4</a></strong> to <a href='https://wandb.ai/team-mykcs/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/team-mykcs/lightning_logs' target=\"_blank\">https://wandb.ai/team-mykcs/lightning_logs</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/team-mykcs/lightning_logs/runs/rdomjq13' target=\"_blank\">https://wandb.ai/team-mykcs/lightning_logs/runs/rdomjq13</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type                   | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model              | Unet                   | 32.5 M\n",
      "1 | loss_fn            | DiceCELoss             | 0     \n",
      "2 | val_accuracy_MACRO | MulticlassAccuracy     | 0     \n",
      "3 | val_accuracy_micro | MulticlassAccuracy     | 0     \n",
      "4 | Dice               | Dice                   | 0     \n",
      "5 | F1                 | MulticlassF1Score      | 0     \n",
      "6 | Jaccard            | MulticlassJaccardIndex | 0     \n",
      "--------------------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.067   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ceb7ae6afd70438db3b8f3af8e073c11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/myk/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MultipleImageDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 65971) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/queues.py:107\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    106\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:257\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:424\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 424\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/multiprocessing/connection.py:931\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 931\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/selectors.py:415\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 415\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 65971) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 运行训练\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtest_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1031\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m   1033\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1057\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1060\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:128\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    127\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 128\u001B[0m     batch, batch_idx, dataloader_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m previous_dataloader_idx \u001B[38;5;241m!=\u001B[39m dataloader_idx:\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;66;03m# the dataloader has changed, notify the logger connector\u001B[39;00m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_dataloader_outputs()\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:133\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatches\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:60\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_profiler()\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _ITERATOR_RETURN:\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n\u001B[1;32m    343\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001B[0m, in \u001B[0;36m_Sequential.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    139\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 142\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterators\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;66;03m# try the next iterator\u001B[39;00m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_next_iterator()\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/anaconda3/envs/seg-test-0401/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 65971) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# 运行训练\n",
    "test_trainer.fit(\n",
    "    model,\n",
    "    datamodule=data_module\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T19:06:39.652936Z",
     "start_time": "2024-04-04T19:06:33.560227Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sweeps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1: Define objective/training function\n",
    "# 2: Define the search space\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [8, 16, 32]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_config,\n",
    "    project=\"test-sweeps02\"\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        model = UNetTestModel(\n",
    "            learning_rate=config.learning_rate,\n",
    "            #             batch_size=config.batch_size,\n",
    "            # 其他参数...\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=150,\n",
    "            callbacks=[lr_monitor, ValidationCallback()],\n",
    "            logger=wandb_logger,\n",
    "            #             check_val_every_n_epoch=10,\n",
    "            # 其他设置...\n",
    "        )\n",
    "        data_module = MOADataModule(data_dir=data_dir, batch_size=config.batch_size)\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep_train, count=10)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T15:12:26.656102Z",
     "iopub.execute_input": "2024-04-04T15:12:26.657004Z",
     "iopub.status.idle": "2024-04-04T16:08:14.263961Z",
     "shell.execute_reply.started": "2024-04-04T15:12:26.656962Z",
     "shell.execute_reply": "2024-04-04T16:08:14.263010Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lightning Style Formal Test"
   ],
   "metadata": {
    "_uuid": "8aa9f836-18b6-4afe-8a97-02aa78f1c528",
    "_cell_guid": "db51d23f-6157-4418-9399-1403d782d90e",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "# 假设你已经定义了 LiTSDataModule\n",
    "data_module = MOADataModule(data_dir=data_dir, batch_size=16)\n",
    "# 初始化模型和训练器\n",
    "model = UNetTestModel(bilinear=False)\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"SMU MOA\", name=\"ResUNetPP50_monaiDiceCELoss_Max150\")\n",
    "wandb_logger = WandbLogger(project=\"UNet-Baseline\", name=\"Origin_UNet_noAtt_Max150_DiceCELoss\")\n",
    "# wandb_logger = WandbLogger()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=150,\n",
    "    #                      fast_dev_run=True, \n",
    "    logger=wandb_logger,\n",
    "    #                   callbacks=[lr_monitor, ValidationCallback()],\n",
    "    callbacks=[lr_monitor],\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    #                   precision='16-mixed',\n",
    ")\n",
    "\n",
    "# 创建 Tuner 对象并运行学习率查找\n",
    "# tuner = Tuner(trainer)\n",
    "# lr_finder = tuner.lr_find(model, datamodule=data_module)\n",
    "\n",
    "# # 可视化找到的学习率\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# display(fig)\n",
    "\n",
    "# # 将建议的学习率设置给模型\n",
    "# new_lr = lr_finder.suggestion()\n",
    "# model.hparams.learning_rate = new_lr\n",
    "\n",
    "\n",
    "# 注意：在此处，你不需要手动更新 DataLoader 的批量大小\n",
    "# 因为 tuner.scale_batch_size 方法已经更新了 LiTSDataModule 中的 batch_size\n",
    "# 你可以检查更新后的批量大小\n",
    "# tuner.scale_batch_size(model, datamodule=data_module, mode=\"power\")\n",
    "# print(f\"Updated batch size: {data_module.batch_size}\")\n",
    "\n",
    "# 使用更新后的学习率继续训练\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "_uuid": "0049db84-14fe-45a1-aef2-75921b97a7c0",
    "_cell_guid": "02f8ebdf-9845-4e4b-9439-0fbefbf50fbd",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-04T16:08:14.265786Z",
     "iopub.execute_input": "2024-04-04T16:08:14.266060Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 确保模型处于评估模式\n",
    "model.eval()\n",
    "print(\"eval activated\")\n",
    "\n",
    "\n",
    "def predict_and_log_images(num_samples=2):\n",
    "    # 假设 test_loader 和 model 已经定义好了，并且 model 已经移动到了适当的设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    test_loader = data_module.test_dataloader()\n",
    "\n",
    "    # 生成随机索引\n",
    "    indices = torch.randperm(len(test_loader.dataset))[:num_samples]\n",
    "    # 调整subplot的大小\n",
    "    fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))  # 每个样本显示3张图（原图、真实掩码、预测掩码）\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = test_loader.dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)  # 添加batch维度并移动到设备\n",
    "        mask = mask.squeeze()  # 移除batch维度（如果有的话）\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(image)\n",
    "            prediction = torch.argmax(pred, dim=1).cpu()  # 获取预测类别并移回CPU\n",
    "\n",
    "        # 显示原始图像\n",
    "        axs[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
    "        axs[i, 0].set_title(f'Original Image {i + 1}')\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # 显示Ground Truth\n",
    "        axs[i, 1].imshow(mask.cpu().numpy(), cmap='gray')\n",
    "        axs[i, 1].set_title(f'True Mask {i + 1}')\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "        # 显示预测掩码\n",
    "        axs[i, 2].imshow(prediction[0].numpy(), cmap='gray')\n",
    "        axs[i, 2].set_title(f'Predicted Mask {i + 1}')\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #     plt.close(fig)  # 防止在notebook中显示图像\n",
    "    return fig"
   ],
   "metadata": {
    "_uuid": "e7c92b10-d357-49d3-a914-e4db61774d96",
    "_cell_guid": "121e8a6e-fabd-45ac-9572-30eb13c443d3",
    "collapsed": false,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 循环十次，每次都记录图像\n",
    "for _ in range(10):\n",
    "    fig = predict_and_log_images(num_samples=2)\n",
    "    wandb.log({\"Predicted Images\": wandb.Image(fig)})"
   ],
   "metadata": {
    "_uuid": "ca2de490-a6d3-442b-ae6f-6ec9c9ed6d70",
    "_cell_guid": "b7ef133e-78ff-4942-b372-18d4909f3531",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-03T07:40:32.977358Z",
     "iopub.execute_input": "2024-04-03T07:40:32.97822Z",
     "iopub.status.idle": "2024-04-03T07:40:50.68459Z",
     "shell.execute_reply.started": "2024-04-03T07:40:32.978189Z",
     "shell.execute_reply": "2024-04-03T07:40:50.683516Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = trainer.test(dataloaders=data_module.val_dataloader())\n",
    "print(results)"
   ],
   "metadata": {
    "_uuid": "3b5689d1-ff8c-4930-88fa-c9dfa38f5195",
    "_cell_guid": "5725cdac-948a-4953-8a35-18186f67bdb1",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-03T05:31:50.942335Z",
     "iopub.execute_input": "2024-04-03T05:31:50.942717Z",
     "iopub.status.idle": "2024-04-03T05:31:51.108384Z",
     "shell.execute_reply.started": "2024-04-03T05:31:50.942684Z",
     "shell.execute_reply": "2024-04-03T05:31:51.106887Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 创建一个新的Artifact，指定其类型为'model'和Artifact的名称\n",
    "artifact = wandb.Artifact('ResUNetPP50_monaiDiceCELoss_Max150', type='model')\n",
    "artifact.add_file('/kaggle/working/SMU-MOA/vq9tunx5/checkpoints/epoch=149-step=4950.ckpt')\n"
   ],
   "metadata": {
    "_uuid": "51fc1ba5-2cb0-4d6c-9869-a2818e01d7a2",
    "_cell_guid": "ddb49941-070c-4aba-8d7b-d77746b33eea",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-01T03:25:52.087368Z",
     "iopub.execute_input": "2024-04-01T03:25:52.088157Z",
     "iopub.status.idle": "2024-04-01T03:25:54.622164Z",
     "shell.execute_reply.started": "2024-04-01T03:25:52.088116Z",
     "shell.execute_reply": "2024-04-01T03:25:54.621037Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 保存Artifact到wandb\n",
    "wandb.log_artifact(artifact)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "_uuid": "b406957a-23ba-4bc5-abe5-2f222a2093aa",
    "_cell_guid": "861d04f7-0fb1-4f03-917b-199d6f54d36b",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-03T07:51:36.429134Z",
     "iopub.execute_input": "2024-04-03T07:51:36.429548Z",
     "iopub.status.idle": "2024-04-03T07:51:36.434822Z",
     "shell.execute_reply.started": "2024-04-03T07:51:36.429512Z",
     "shell.execute_reply": "2024-04-03T07:51:36.433724Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
