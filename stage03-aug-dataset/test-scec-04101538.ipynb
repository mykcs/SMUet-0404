{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7972942,"sourceType":"datasetVersion","datasetId":4691017},{"sourceId":8062127,"sourceType":"datasetVersion","datasetId":4741616}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yufang18/test-scec?scriptVersionId=171304885\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Pip 安装命令\n!pip install segmentation-models-pytorch -q\n# !pip install lightning\n!pip install wandb -U -q\n!pip install monai  -q\n!git clone https://github.com/by-liu/SegLossBias.git  -q\n!pip install yacs  -q\n\n# !pip install ipywidgets\n# !pip install albumentations\n# !pip install nibabel","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:53:39.325315Z","iopub.execute_input":"2024-04-10T07:53:39.325606Z","iopub.status.idle":"2024-04-10T07:54:42.089179Z","shell.execute_reply.started":"2024-04-10T07:53:39.325578Z","shell.execute_reply":"2024-04-10T07:54:42.087863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# 导入的库\nimport sys\n\nsys.path.append('/kaggle/working/SegLossBias')\n\nimport IPython\n\nfrom matplotlib.patches import Patch, Rectangle\nfrom IPython.display import display\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import LightningDataModule\n\nimport matplotlib.pyplot as plt\n# from fastai.losses import *\nimport torchmetrics\nimport wandb\n# from lightning.pytorch.loggers import WandbLogger\nfrom pytorch_lightning.core.mixins import HyperparametersMixin\nfrom pytorch_lightning.callbacks import Callback\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\nfrom pytorch_lightning.loggers import WandbLogger\n\nIPython.display.clear_output()\n\nprint(\"Envirionment Set Up.\")","metadata":{"_uuid":"36007df4-0fc3-4938-9a09-f54d22d06e5f","_cell_guid":"c62342c1-1564-439b-b5f2-8bc2c518bdaf","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-10T07:54:42.091179Z","iopub.execute_input":"2024-04-10T07:54:42.091493Z","iopub.status.idle":"2024-04-10T07:54:54.188155Z","shell.execute_reply.started":"2024-04-10T07:54:42.091464Z","shell.execute_reply":"2024-04-10T07:54:54.187193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and Augment Setting","metadata":{"_uuid":"5cf48cd2-93fe-4aac-ada9-1a19cd8b2e56","_cell_guid":"0134c009-4039-4716-893f-56425743cab6","trusted":true}},{"cell_type":"code","source":"# 定义数据增强\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.2),\n    A.VerticalFlip(p=0.2),\n    A.RandomRotate90(),\n    #     A.RandomBrightnessContrast(p=0.2),\n    A.ElasticTransform(p=0.3, alpha=120, sigma=120 * 0.3, alpha_affine=120 * 0.2),\n    A.RandomSizedCrop(min_max_height=(128, 256), height=256, width=256, p=0.3),\n    # A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    # A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])\n\n\ndef adjust_window(image, window_center, window_width):\n    \"\"\"\n    调整CT图像的窗宽窗位。\n    :param image: 输入的图像数组。\n    :param window_center: 窗位（WC）。\n    :param window_width: 窗宽（WW）。\n    :return: 调整窗宽窗位后的图像。\n    \"\"\"\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    windowed_img = np.clip(image, img_min, img_max)\n    # print(windowed_img.dtype) # NOW its float64\n    return windowed_img\n\n\nclass MultipleImageDataset(Dataset):\n    def __init__(self, image_paths, label_paths, transform=None):\n        \"\"\"\n        image_paths: 图像文件路径列表\n        label_paths: 标签文件路径列表\n        transform: 应用于图像和标签的转换操作\n        \"\"\"\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n        self.transform = transform\n\n    def __len__(self):\n        # 假设图像和标签列表长度相等\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):  # dataloader获取每个数据都会用到这个函数，所以你应当在这里实现你需要的\n        # 预处理等等步骤\n        image = (np.load(self.image_paths[idx]))['arr_0']\n        #         print(image.shape)\n        label = (np.load(self.label_paths[idx]))['arr_0']\n        #         print(label.shape)\n\n        image = adjust_window(image, window_center=40, window_width=400)\n\n        if self.transform:\n            #             image = image.astype(np.float32)\n            augmented = self.transform(image=image, mask=label)\n            image = augmented['image']\n            #             print(\"image aug\")\n            label = augmented['mask']\n            image = image.float()\n            label = label.long()\n\n        label = label.long()\n        image = (image - image.min()) / (image.max() - image.min())\n        return image.float(), label.long()\n\n\n######################################################################################################################\nclass MOADataModule(LightningDataModule):\n    def __init__(self, data_dir: str, batch_size: int = 16):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.train_transform = train_transform\n        self.val_transform = val_transform\n\n    def setup(self, stage=None):\n        image_dir = os.path.join(self.data_dir, 'image_npz')  # 注意这里路径的更正\n        label_dir = os.path.join(self.data_dir, 'mask_npz')\n\n        # 读取文件路径\n        image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.npz')])\n        label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.npz')])\n\n        # 划分训练集、验证集、测试集\n        train_size = int(0.8 * len(image_files))\n        val_size = int(0.1 * len(image_files))\n\n        self.train_image_paths = image_files[:train_size]\n        self.val_image_paths = image_files[train_size:train_size + val_size]\n        self.test_image_paths = image_files[train_size + val_size:]\n\n        self.train_label_paths = label_files[:train_size]\n        self.val_label_paths = label_files[train_size:train_size + val_size]\n        self.test_label_paths = label_files[train_size + val_size:]\n\n    def train_dataloader(self):\n        train_dataset = MultipleImageDataset(self.train_image_paths, self.train_label_paths,\n                                             transform=self.train_transform)\n        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n\n    def val_dataloader(self):\n        val_dataset = MultipleImageDataset(self.val_image_paths, self.val_label_paths, transform=self.val_transform)\n        return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    def test_dataloader(self):\n        test_dataset = MultipleImageDataset(self.test_image_paths, self.test_label_paths, transform=self.val_transform)\n        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n\n# 定义数据集和数据加载器\nold_data_dir = '/kaggle/input/rawniidataset/SMU_Dataset'\naug_data_dir = '/kaggle/input/aug-dataset-for-fine-tune/AUG_dataset'\ndata_dir = aug_data_dir \n\ndef predict_and_log_images(num_samples=2):\n    # 假设 test_loader 和 model 已经定义好了，并且 model 已经移动到了适当的设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    test_loader = data_module.test_dataloader()\n\n    # 生成随机索引\n    indices = torch.randperm(len(test_loader.dataset))[:num_samples]\n\n    # 调整subplot的大小\n    fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))  # 每个样本显示3张图（原图、真实掩码、预测掩码）\n    cmap = plt.get_cmap('tab20')  # 获取颜色映射\n    for i, idx in enumerate(indices):\n        image, mask = test_loader.dataset[idx]\n        class_labels = np.unique(mask)  # 获取类别标签\n        colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n        image = image.unsqueeze(0).to(device)  # 添加batch维度并移动到设备\n        mask = mask.squeeze()  # 移除batch维度（如果有的话）\n\n        with torch.no_grad():\n            pred = model(image)\n            prediction = torch.argmax(pred, dim=1).cpu()  # 获取预测类别并移回CPU\n\n        # 显示原始图像\n        axs[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n        axs[i, 0].set_title(f'Original Image {i + 1}')\n        axs[i, 0].axis('off')\n\n        # 显示Ground Truth\n        axs[i, 1].imshow(mask.cpu().numpy(), cmap='tab20')\n        axs[i, 1].set_title(f'True Mask {i + 1}')\n        axs[i, 1].axis('off')\n\n        # 显示预测掩码\n        axs[i, 2].imshow(prediction[0].numpy(), cmap='tab20')\n        axs[i, 2].set_title(f'Predicted Mask {i + 1}')\n        axs[i, 2].axis('off')\n\n    legend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\n    fig.legend(handles=legend_elements, loc='upper center', ncol=len(class_labels), title=\"Classes\")\n    plt.tight_layout()\n    plt.close(fig)  # 防止在notebook中显示图像\n    return fig\n\n\nclass ValidationCallback(Callback):\n    def on_validation_epoch_end(self, trainer, pl_module):\n        # 每10个epoch执行一次\n        if (trainer.current_epoch + 1) % 5 == 0:\n            fig = predict_and_log_images(num_samples=2)\n            # 用wandb记录图像，或进行其他操作\n            wandb.log({\"Validation Callback Predicted Images\": wandb.Image(fig)})","metadata":{"_uuid":"d3ebdfad-a463-469d-812a-9214d9338d08","_cell_guid":"cfd87ac0-9db7-4743-9831-9f75542fc41c","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-10T07:54:54.189753Z","iopub.execute_input":"2024-04-10T07:54:54.190225Z","iopub.status.idle":"2024-04-10T07:54:54.222297Z","shell.execute_reply.started":"2024-04-10T07:54:54.190198Z","shell.execute_reply":"2024-04-10T07:54:54.221349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (Opt) Data Pre-Check","metadata":{"_uuid":"877d1009-7e88-4dff-8d32-34f278b9ba2f","_cell_guid":"f5aba93a-1cc3-4f13-9763-b1bbe5b31453","trusted":true}},{"cell_type":"code","source":"# 初始化数据模块\ndata_module = MOADataModule(data_dir=data_dir, batch_size=16)\n\n# 设置数据模块（准备数据）\ndata_module.setup()\n\n# 获取训练数据加载器\ntrain_loader = data_module.train_dataloader()\n\n# 从数据加载器中抽取一批数据\nimages, labels = next(iter(train_loader))\n\n# 选择要展示的图像数量\nnum_images_to_show = 4\n\n# 创建图表来展示图像和对应的掩码\nfig, axs = plt.subplots(num_images_to_show, 3, figsize=(15, num_images_to_show * 5))\nclass_labels = np.unique(labels)  # 获取类别标签\ncmap = plt.get_cmap('tab20')  # 获取颜色映射\ncolors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n\nfor i in range(num_images_to_show):\n    img = images[i].squeeze().numpy()  # 假设图像和掩码都只有一个通道\n    lbl = labels[i].squeeze().numpy()\n    overlay = np.ma.masked_where(lbl == 0, lbl)\n\n    axs[i, 0].imshow(img, cmap='gray')\n    axs[i, 0].set_title('Image')\n    axs[i, 0].axis('off')\n\n    axs[i, 1].imshow(lbl, cmap='tab20')\n    axs[i, 1].set_title('Mask')\n    axs[i, 1].axis('off')\n\n    axs[i, 2].imshow(img, cmap='gray')\n    axs[i, 2].imshow(overlay, cmap='tab20', alpha=0.5)\n    axs[i, 2].set_title('Overlay')\n    axs[i, 2].axis('off')\n\n    print(f\"Image shape: {img.shape}\")\n    print(f\"Label shape: {lbl.shape}\")\n    print(f\"Image M&m Value: {img.max(), img.min()}\")\n    print(f\"Label Unique: {np.unique(lbl)}\")\n\nlegend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\nfig.legend(handles=legend_elements, loc='upper center', ncol=len(class_labels), title=\"Classes\")\n\nplt.tight_layout()\nplt.show()\nprint('Show Time!')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:54:54.224502Z","iopub.execute_input":"2024-04-10T07:54:54.224797Z","iopub.status.idle":"2024-04-10T07:55:00.43164Z","shell.execute_reply.started":"2024-04-10T07:54:54.224772Z","shell.execute_reply":"2024-04-10T07:55:00.430617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom DiceCELossWithKL based on SegLossBias\nhttps://github.com/by-liu/SegLossBias.git","metadata":{}},{"cell_type":"code","source":"from seglossbias.modeling.compound_losses import *\nfrom monai.losses.dice import *\nfrom torch.nn.modules.loss import _Loss\n\n\nclass DiceCELossWithKL(_Loss):  # 添加KL散度\n    def __init__(\n            self,\n            mode: str = 'MULTICLASS_MODE',\n            include_background: bool = True,\n            to_onehot_y: bool = False,\n            sigmoid: bool = False,\n            softmax: bool = False,\n            other_act: Callable | None = None,\n            squared_pred: bool = False,\n            jaccard: bool = False,\n            reduction: str = \"mean\",\n            smooth_nr: float = 1e-5,\n            smooth_dr: float = 1e-5,\n            batch: bool = False,\n            ce_weight: torch.Tensor | None = None,\n            weight: torch.Tensor | None = None,\n            lambda_dice: float = 1.0,\n            lambda_ce: float = 1.0,\n            lambda_kl: float = 1.0,  # KL divergence weight\n            temp: float = 10.0\n    ) -> None:\n\n        super().__init__()\n        self.mode = mode  # 设置mode\n        self.temp = temp  # 设置temp\n        reduction = look_up_option(reduction, DiceCEReduction).value\n        weight = ce_weight if ce_weight is not None else weight\n        dice_weight: torch.Tensor | None\n        if weight is not None and not include_background:\n            dice_weight = weight[1:]\n        else:\n            dice_weight = weight\n        self.dice = DiceLoss(\n            include_background=include_background,\n            to_onehot_y=to_onehot_y,\n            sigmoid=sigmoid,\n            softmax=softmax,\n            other_act=other_act,\n            squared_pred=squared_pred,\n            jaccard=jaccard,\n            reduction=reduction,\n            smooth_nr=smooth_nr,\n            smooth_dr=smooth_dr,\n            batch=batch,\n            weight=dice_weight,\n        )\n        self.cross_entropy = nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n        self.binary_cross_entropy = nn.BCEWithLogitsLoss(pos_weight=weight, reduction=reduction)\n        if lambda_dice < 0.0:\n            raise ValueError(\"lambda_dice should be no less than 0.0.\")\n        if lambda_ce < 0.0:\n            raise ValueError(\"lambda_ce should be no less than 0.0.\")\n        self.lambda_dice = lambda_dice\n        self.lambda_ce = lambda_ce\n        self.old_pt_ver = not pytorch_after(1, 10)\n        self.lambda_kl = lambda_kl\n\n    def ce(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute CrossEntropy loss for the input logits and target.\n        Will remove the channel dim according to PyTorch CrossEntropyLoss:\n        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?#torch.nn.CrossEntropyLoss.\n\n        \"\"\"\n        n_pred_ch, n_target_ch = input.shape[1], target.shape[1]\n        if n_pred_ch != n_target_ch and n_target_ch == 1:\n            target = torch.squeeze(target, dim=1)\n            target = target.long()\n        elif self.old_pt_ver:\n            warnings.warn(\n                f\"Multichannel targets are not supported in this older Pytorch version {torch.__version__}. \"\n                \"Using argmax (as a workaround) to convert target to a single channel.\"\n            )\n            target = torch.argmax(target, dim=1)\n        elif not torch.is_floating_point(target):\n            target = target.to(dtype=input.dtype)\n\n        return self.cross_entropy(input, target)  # type: ignore[no-any-return]\n\n    def bce(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute Binary CrossEntropy loss for the input logits and target in one single class.\n\n        \"\"\"\n        if not torch.is_floating_point(target):\n            target = target.to(dtype=input.dtype)\n\n        return self.binary_cross_entropy(input, target)  # type: ignore[no-any-return]\n\n    def kl_div(self, p, q):\n        \"\"\" Calculate KL divergence \"\"\"\n        kl = p * torch.log((p + 1e-10) / (q + 1e-10))\n        return kl.sum()\n\n    def convert_to_one_hot(self, targets, num_classes):\n        targets = torch.squeeze(targets, dim=1)\n\n        # 转换为one-hot [batch_size, height, width, num_classes]\n        targets_one_hot = F.one_hot(targets, num_classes=num_classes)\n\n        # 转置维度以匹配输出 [batch_size, num_classes, height, width]\n        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2)\n\n        return targets_one_hot\n\n    def calculate_gt_proportion(self, targets_one_hot):\n        # targets_one_hot: [batch_size, num_classes, height, width]\n        # 计算每个类别的总像素数\n        class_totals = targets_one_hot.sum(dim=[0, 2, 3])  # 按照批次和空间维度聚合\n\n        # 避免除以零，使用一个小的epsilon\n        epsilon = 1e-8\n        # 计算每个类别的比例\n        gt_proportion = class_totals / (class_totals.sum() + epsilon)\n\n        return gt_proportion\n\n    def calculate_pred_proportion(self, preds):\n        # preds: [batch_size, num_classes, height, width], 模型输出的概率分布\n        # 计算每个类别的预测概率总和\n        pred_totals = preds.sum(dim=[0, 2, 3])  # 按照批次和空间维度聚合\n\n        # 避免除以零，使用一个小的epsilon\n        epsilon = 1e-8\n        # 计算每个类别的预测概率比例\n        pred_proportion = pred_totals / (pred_totals.sum() + epsilon)\n\n        return pred_proportion\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            input: the shape should be BNH[WD].\n            target: the shape should be BNH[WD] or B1H[WD].\n\n        Raises:\n            ValueError: When number of dimensions for input and target are different.\n            ValueError: When number of channels for target is neither 1 nor the same as input.\n\n        \"\"\"\n        if len(input.shape) != len(target.shape):\n            raise ValueError(\n                \"the number of dimensions for input and target should be the same, \"\n                f\"got shape {input.shape} and {target.shape}.\"\n            )\n\n        dice_loss = self.dice(input, target)\n        ce_loss = self.ce(input, target) if input.shape[1] != 1 else self.bce(input, target)\n        preds = F.softmax(input, dim=1)\n        # 在你的模型或损失函数中使用\n        # 假设 num_classes 是你模型输出的类别数\n        target_one_hot = self.convert_to_one_hot(target, num_classes=input.size(1))\n        gt_proportion = self.calculate_gt_proportion(target_one_hot)\n        pred_proportion = self.calculate_pred_proportion(preds)\n\n        # Print the values of the losses\n        #         print(f\"Dice Loss: {dice_loss.item()}\")\n        #         print(f\"CE Loss: {ce_loss.item()}\")\n        #         print(f\"KL Divergence Regularizer: {regularizer.item()}\")\n        kl_loss = self.kl_div(gt_proportion, pred_proportion)\n\n        total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_ce * ce_loss + self.lambda_kl * kl_loss\n        #         print(f\"gt: {gt_proportion}\")\n        #         print(f\"pred: {pred_proportion}\")\n        #         print(f\"kl_loss: {kl_loss}\")\n        #         print(f\"total_loss: {total_loss}\")\n        return total_loss\n\n# loss = self.loss_fn(outputs, labels.unsqueeze(1))\n# Images shape: torch.Size([1, 1, 256, 256])\n# Labels shape: torch.Size([1, 256, 256])\n# Outputs shape: torch.Size([1, 14, 256, 256])\n# Labels with unsqueezed dim shape: torch.Size([1, 1, 256, 256])\n# Predictions shape: torch.Size([1, 256, 256])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:55:00.4335Z","iopub.execute_input":"2024-04-10T07:55:00.433986Z","iopub.status.idle":"2024-04-10T07:55:42.210993Z","shell.execute_reply.started":"2024-04-10T07:55:00.433942Z","shell.execute_reply":"2024-04-10T07:55:42.210173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Setting","metadata":{"_uuid":"612e9ff5-ea09-4a38-bc45-d077638c795f","_cell_guid":"13f85698-ebef-40de-a063-58b836eb7980","trusted":true}},{"cell_type":"code","source":"class UNetTestModel(pl.LightningModule, HyperparametersMixin):\n    def __init__(\n            self,\n            encoder_name='resnet50',\n            encoder_weights='imagenet',\n            in_channels=1,\n            classes=14,\n            #         loss_fn=monai.losses.FocalLoss(use_softmax=True, to_onehot_y=True, include_background=False),\n            loss_fn=DiceCELossWithKL(softmax=True, lambda_dice=0.85, lambda_ce=0.15, to_onehot_y=True,\n                                     include_background=True),\n            #         loss_fn=monai.losses.DiceCELoss(softmax=True, lambda_dice=0.85, lambda_ce=0.15, to_onehot_y=True),\n\n            #         loss_fn=monai.losses.DiceLoss( to_onehot_y=True),\n            loss_function='DiceCELossWithKL',\n            learning_rate=3e-3,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        ###################### model #########################\n        self.model = smp.Unet(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels,\n            classes=classes,\n            decoder_attention_type='scse',\n        )\n        ###################### loss ##########################\n        self.loss_fn = loss_fn\n        ###################### metrics ######################\n        #         self.train_accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes, average='micro', ignore_index=0)\n        self.val_accuracy_MACRO = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes,\n                                                                       average='macro', ignore_index=0)\n        self.val_accuracy_micro = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes,\n                                                                       average='micro', ignore_index=0)\n        self.val_accuracy_classwise = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=classes,\n                                                                           average='none', ignore_index=0)\n        self.Dice = torchmetrics.classification.Dice(multiclass=True, num_classes=classes, average='micro',\n                                                     ignore_index=0)\n        self.F1 = torchmetrics.classification.MulticlassF1Score(num_classes=classes, average=\"micro\", ignore_index=0)\n        self.Jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=classes, average=\"micro\",\n                                                                          ignore_index=0)\n\n    # 定义前向传播\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self.forward(images)\n        loss = self.loss_fn(outputs, labels.unsqueeze(1))\n        self.log('train_loss', loss, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self.forward(images)\n        loss = self.loss_fn(outputs, labels.unsqueeze(1))\n        preds = torch.argmax(outputs, dim=1)\n        #         print(f\"Images shape: {images.shape}\")\n        #         print(f\"Labels shape: {labels.shape}\")\n        #         print(f\"Outputs shape: {outputs.shape}\")\n        #         print(f\"Labels with unsqueezed dim shape: {labels.unsqueeze(1).shape}\")\n        #         print(f\"Predictions shape: {preds.shape}\")\n        #         print(labels_one_hot.shape)\n        acc_micro = self.val_accuracy_micro(preds, labels)\n        acc_MACRO = self.val_accuracy_MACRO(preds, labels)\n        Dice = self.Dice(preds, labels)\n        F1 = self.F1(preds, labels)\n        Jaccard = self.Jaccard(preds, labels)\n        acc = self.val_accuracy_classwise(preds, labels)\n\n        self.log('val_loss', loss, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_accuracy_micro', acc_micro, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_accuracy_MACRO', acc_MACRO, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_F1', F1, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_Dice', Dice, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_Jaccard', Jaccard, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n\n        self.log('val_acc_4', acc[4], on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_acc_5', acc[5], on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_acc_10', acc[10], on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_acc_12', acc[12], on_step=True, on_epoch=False, logger=True, prog_bar=True)\n        self.log('val_acc_13', acc[13], on_step=True, on_epoch=False, logger=True, prog_bar=True)\n\n    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure, **kwargs):\n        # 调用优化器的step方法前执行自定义操作\n        # 比如实现学习率热启动\n        if self.trainer.global_step < 50:\n            lr_scale = min(1.0, float(self.trainer.global_step + 1) / 50)\n            for pg in optimizer.param_groups:\n                pg[\"lr\"] = lr_scale * self.hparams.learning_rate\n        # 调用优化器的step方法来更新模型参数\n        optimizer.step(closure=optimizer_closure)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n        #         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9, verbose=True)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0.000001, last_epoch=-1)\n        #         print(self.hparams.learning_rate)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'interval': 'epoch',  # 指定更新学习率的间隔单位为'epoch'\n                'frequency': 1,  # 每个epoch更新一次学习率\n            }\n        }","metadata":{"_uuid":"ac36333a-0d4f-494b-a95c-889fc8ca64d9","_cell_guid":"e855577b-fc8c-4409-b4d6-0f82ee37f1d3","execution":{"iopub.status.busy":"2024-04-10T07:58:08.784726Z","iopub.execute_input":"2024-04-10T07:58:08.785107Z","iopub.status.idle":"2024-04-10T07:58:08.809049Z","shell.execute_reply.started":"2024-04-10T07:58:08.785075Z","shell.execute_reply":"2024-04-10T07:58:08.808198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_classes = [4, 5, 10, 12, 13]\n\ndef draw_bounding_boxes(ax, mask, class_ids):\n    \"\"\" 在给定的轴上绘制边界框，突出显示特定类别 \"\"\"\n    special_classes = [4, 5, 10, 12, 13]\n    for class_id in class_ids:\n        positions = np.argwhere(mask == class_id)\n        if positions.size > 0:\n            xmin, xmax = positions[:, 0].min(), positions[:, 0].max()\n            ymin, ymax = positions[:, 1].min(), positions[:, 1].max()\n            rect = Rectangle((ymin, xmin), ymax - ymin, xmax - xmin, linewidth=2, edgecolor='red', facecolor='none')\n            ax.add_patch(rect)\n            ax.text(ymin, xmin, f'Class {class_id}', color='red', fontsize=12, va='top', ha='left')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:57:46.331914Z","iopub.execute_input":"2024-04-10T07:57:46.332914Z","iopub.status.idle":"2024-04-10T07:57:46.340765Z","shell.execute_reply.started":"2024-04-10T07:57:46.332866Z","shell.execute_reply":"2024-04-10T07:57:46.339703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 获取训练数据加载器\ntrain_loader = data_module.train_dataloader()\n\n# 从数据加载器中抽取一批数据\nimages, labels = next(iter(train_loader))\n\n# 选择要展示的图像数量\nnum_images_to_show = 4\n\n# 创建图表来展示图像和对应的掩码\nfig, axs = plt.subplots(num_images_to_show, 3, figsize=(15, num_images_to_show * 5))\nclass_labels = np.unique(labels)  # 获取类别标签\ncmap = plt.get_cmap('tab20')  # 获取颜色映射\ncolors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n\nfor i in range(num_images_to_show):\n    img = images[i].squeeze().numpy()  # 假设图像和掩码都只有一个通道\n    lbl = labels[i].squeeze().numpy()\n    overlay = np.ma.masked_where(lbl == 0, lbl)\n    \n    axs[i, 0].imshow(img, cmap='gray')\n    axs[i, 0].set_title('Image')\n    axs[i, 0].axis('off')\n    \n    axs[i, 1].imshow(lbl, cmap='tab20')\n    axs[i, 1].set_title('Mask')\n    axs[i, 1].axis('off')\n    \n    axs[i, 2].imshow(img, cmap='gray')\n    axs[i, 2].imshow(overlay, cmap='tab20', alpha=0.5)\n    axs[i, 2].set_title('Overlay')\n    axs[i, 2].axis('off')\n    \n    draw_bounding_boxes(axs[i, 1], lbl, special_classes)\n    draw_bounding_boxes(axs[i, 2], overlay, special_classes)     \n    \n    print(f\"Image shape: {img.shape}\")\n    print(f\"Label shape: {lbl.shape}\")\n    print(f\"Image M&m Value: {img.max(), img.min()}\")\n    print(f\"Label Unique: {np.unique(lbl)}\")\n    \nlegend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\nfig.legend(handles=legend_elements, loc='upper center', ncol=len(class_labels), title=\"Classes\")\n\n\nplt.tight_layout()\nplt.show()\nprint('Show Time!')","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:57:48.645818Z","iopub.execute_input":"2024-04-10T07:57:48.646226Z","iopub.status.idle":"2024-04-10T07:57:53.882125Z","shell.execute_reply.started":"2024-04-10T07:57:48.646198Z","shell.execute_reply":"2024-04-10T07:57:53.880968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (Opt) Hyper Params Check","metadata":{"_uuid":"7357d769-1c7b-4ae0-b717-68fe4d9524bc","_cell_guid":"f00537a2-4837-4a41-94fe-819c9ce7b959","trusted":true}},{"cell_type":"code","source":"from torchinfo import summary\n\nmodel = UNetTestModel()\nprint(summary(model, input_size=(1, 1, 256, 256)))\n","metadata":{"_uuid":"54a3656f-6ee2-41bb-92cc-2f278e3ed7d7","_cell_guid":"e6d612dd-14b3-4d64-960e-78b581b1d7e3","scrolled":true,"execution":{"iopub.status.busy":"2024-04-10T07:58:16.889874Z","iopub.execute_input":"2024-04-10T07:58:16.890607Z","iopub.status.idle":"2024-04-10T07:58:19.053989Z","shell.execute_reply.started":"2024-04-10T07:58:16.890571Z","shell.execute_reply":"2024-04-10T07:58:19.053042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.hparams)\n# def print_model_details(model, indent=0):\n#     for name, child in model.named_children():\n#         print(\" \" * indent, name, child)\n#         print_model_details(child, indent+4)\n\n# print_model_details(model)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:22.259363Z","iopub.execute_input":"2024-04-10T07:58:22.259736Z","iopub.status.idle":"2024-04-10T07:58:22.265247Z","shell.execute_reply.started":"2024-04-10T07:58:22.259705Z","shell.execute_reply":"2024-04-10T07:58:22.264292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torchviz\n\n# from torchviz import make_dot\n\n# x = torch.randn(1, 1, 256, 256)  # 生成一个随机输入\n# y = model(x)\n# make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_model\", format=\"png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# wandb login","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\nwandb.login(key=secret_value_0)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:24.89606Z","iopub.execute_input":"2024-04-10T07:58:24.896431Z","iopub.status.idle":"2024-04-10T07:58:27.613717Z","shell.execute_reply.started":"2024-04-10T07:58:24.896402Z","shell.execute_reply":"2024-04-10T07:58:27.612836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (Opt) Pre-Train Test","metadata":{"_uuid":"7e5a9389-57b6-4a07-b428-b41fcc987525","_cell_guid":"de47c7ee-1eee-4ee1-93f6-d6b43df86981","trusted":true}},{"cell_type":"code","source":"data_module = MOADataModule(data_dir=aug_data_dir, batch_size=1)\n# avail test\nmodel = UNetTestModel()\nlr_monitor = LearningRateMonitor(logging_interval='step')\n############################################## fastrun ###################################################\ntrainer = pl.Trainer(\n    max_epochs=20,\n    fast_dev_run=True,\n    # callbacks=[lr_monitor, ValidationCallback()],\n    # check_val_every_n_epoch=10, \n)\n","metadata":{"_uuid":"3c026544-ec35-408c-9c4a-8840ba0d8202","_cell_guid":"37e59d03-78db-479e-954e-7e8fb28a80a3","execution":{"iopub.status.busy":"2024-04-10T07:58:27.615565Z","iopub.execute_input":"2024-04-10T07:58:27.616411Z","iopub.status.idle":"2024-04-10T07:58:29.159484Z","shell.execute_reply.started":"2024-04-10T07:58:27.616371Z","shell.execute_reply":"2024-04-10T07:58:29.158708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(\n    model,\n    datamodule=data_module\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:32.781127Z","iopub.execute_input":"2024-04-10T07:58:32.781806Z","iopub.status.idle":"2024-04-10T07:58:35.683417Z","shell.execute_reply.started":"2024-04-10T07:58:32.781772Z","shell.execute_reply":"2024-04-10T07:58:35.682041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndata_module = MOADataModule(data_dir=data_dir, batch_size=1)\n# learnability test\nwandb_logger_test = WandbLogger()\n# # 初始化Trainer，设置overfit_batches来过拟合一小部分数据\n# lr_monitor = LearningRateMonitor(logging_interval='step')\ntest_trainer = pl.Trainer(\n    overfit_batches=1,\n    logger=wandb_logger_test,\n    #                           callbacks=[lr_monitor, ValidationCallback()], \n    check_val_every_n_epoch=1\n)\n\n# # 或者，使用10个批次的数据来过拟合\n# test_trainer = pl.Trainer(overfit_batches=10, logger=wandb_logger_test)\n\"\"\"","metadata":{"_uuid":"273bd454-c583-441a-8b86-d5b109f13028","_cell_guid":"4417d2cf-aade-490c-9cf2-4aed7f9bb6f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# 运行训练\ntest_trainer.fit(\n    model,\n    datamodule=data_module\n)\n\"\"\"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.finish()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sweeps","metadata":{}},{"cell_type":"code","source":"\"\"\"\nsweep_config = {\n    'method': 'random',\n    'metric': {\n        'name': 'val_loss',\n        'goal': 'minimize'},\n    'parameters': {\n        'learning_rate': {\n            'min': 0.0001,\n            'max': 0.1},\n        'batch_size': {\n            'values': [16, 32, 64]}\n    }\n}\nwandb_logger = WandbLogger()\nsweep_id = wandb.sweep(sweep_config, project=\"test sweeps2\")\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n\ndef sweep_train():\n    with wandb.init() as run:\n        config = wandb.config\n        model = UNetTestModel(\n            learning_rate=config.learning_rate,\n            #             batch_size=config.batch_size,\n            # 其他参数...\n        )\n        trainer = pl.Trainer(\n            max_epochs=150,\n            callbacks=[lr_monitor, ValidationCallback()],\n            logger=wandb_logger,\n            #             check_val_every_n_epoch=10,\n            # 其他设置...\n        )\n        data_module = MOADataModule(data_dir=data_dir, batch_size=config.batch_size)\n        trainer.fit(model, datamodule=data_module)\n\n\nwandb.agent(sweep_id, function=sweep_train, count=10)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lightning Style Formal Test","metadata":{"_uuid":"8aa9f836-18b6-4afe-8a97-02aa78f1c528","_cell_guid":"db51d23f-6157-4418-9399-1403d782d90e","trusted":true}},{"cell_type":"code","source":"lr_monitor = LearningRateMonitor(logging_interval='step')\n# 假设你已经定义了 LiTSDataModule\ndata_module = MOADataModule(data_dir=aug_data_dir, batch_size=32)\n# 初始化模型和训练器\nmodel = UNetTestModel()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:37.915578Z","iopub.execute_input":"2024-04-10T07:58:37.916105Z","iopub.status.idle":"2024-04-10T07:58:38.805106Z","shell.execute_reply.started":"2024-04-10T07:58:37.916066Z","shell.execute_reply":"2024-04-10T07:58:38.804281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb_logger = WandbLogger(project=\"SMU MOA\", name=\"ResUNetPP50_monaiDiceCELoss_Max150\")\nwandb_logger = WandbLogger(\n    project=\"UNet_Compare\",\n    name=\"test-scec\"\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:42.782651Z","iopub.execute_input":"2024-04-10T07:58:42.783534Z","iopub.status.idle":"2024-04-10T07:58:42.878139Z","shell.execute_reply.started":"2024-04-10T07:58:42.783501Z","shell.execute_reply":"2024-04-10T07:58:42.877248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    max_epochs=200,\n    #                      fast_dev_run=True, \n    logger=wandb_logger,\n    callbacks=[lr_monitor, ValidationCallback()],\n    #                   callbacks=[lr_monitor],\n    log_every_n_steps=1,\n    check_val_every_n_epoch=1,\n    #                   precision='16-mixed',\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:44.823603Z","iopub.execute_input":"2024-04-10T07:58:44.824452Z","iopub.status.idle":"2024-04-10T07:58:44.896733Z","shell.execute_reply.started":"2024-04-10T07:58:44.824417Z","shell.execute_reply":"2024-04-10T07:58:44.89605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 创建 Tuner 对象并运行学习率查找","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom pytorch_lightning.tuner import Tuner\n\n# 创建 Tuner 对象并运行学习率查找\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(model, datamodule=data_module)\n\n# 可视化找到的学习率\nfig = lr_finder.plot(suggest=True)\ndisplay(fig)\n\"\"\"","metadata":{"_uuid":"0049db84-14fe-45a1-aef2-75921b97a7c0","_cell_guid":"02f8ebdf-9845-4e4b-9439-0fbefbf50fbd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# 将建议的学习率设置给模型\nnew_lr = lr_finder.suggestion()\nmodel.hparams.learning_rate = new_lr\n\n# 注意：在此处，你不需要手动更新 DataLoader 的批量大小\n# 因为 tuner.scale_batch_size 方法已经更新了 LiTSDataModule 中的 batch_size\n# 你可以检查更新后的批量大小\ntuner.scale_batch_size(model, datamodule=data_module, mode=\"power\")\nprint(f\"Updated batch size: {data_module.batch_size}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用更新后的学习率继续训练\ntrainer.fit(\n    model,\n    datamodule=data_module\n)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T07:58:48.349999Z","iopub.execute_input":"2024-04-10T07:58:48.35074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##################################################### EVA ###################################################\n# 确保模型处于评估模式\nmodel.eval()\nprint(\"eval activated\")\n\n# # 循环十次，每次都记录图像\nfor _ in range(10):\n    fig = predict_and_log_images(num_samples=2)\n    wandb.log({\"Predicted Images 2\": wandb.Image(fig)})\nprint(\"images logged\")\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_and_log_images(num_samples=2):\n\n#     # 假设 test_loader 和 model 已经定义好了，并且 model 已经移动到了适当的设备\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     model.to(device)\n#     test_loader = data_module.test_dataloader()\n\n#     # 生成随机索引\n#     indices = torch.randperm(len(test_loader.dataset))[:num_samples]\n#     # 调整subplot的大小\n#     fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))  # 每个样本显示3张图（原图、真实掩码、预测掩码）\n\n#     for i, idx in enumerate(indices):\n#         image, mask = test_loader.dataset[idx]\n#         image = image.unsqueeze(0).to(device)  # 添加batch维度并移动到设备\n#         mask = mask.squeeze()  # 移除batch维度（如果有的话）\n\n#         with torch.no_grad():\n#             pred = model(image)\n#             prediction = torch.argmax(pred, dim=1).cpu()  # 获取预测类别并移回CPU\n\n#         # 显示原始图像\n#         axs[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n#         axs[i, 0].set_title(f'Original Image {i+1}')\n#         axs[i, 0].axis('off')\n\n#         # 显示Ground Truth\n#         axs[i, 1].imshow(mask.cpu().numpy(), cmap='viridis')\n#         axs[i, 1].set_title(f'True Mask {i+1}')\n#         axs[i, 1].axis('off')\n\n#         # 显示预测掩码\n#         axs[i, 2].imshow(prediction[0].numpy(), cmap='viridis')\n#         axs[i, 2].set_title(f'Predicted Mask {i+1}')\n#         axs[i, 2].axis('off')\n\n#     plt.tight_layout()\n#     plt.close(fig)  # 防止在notebook中显示图像\n#     return fig\n# 循环十次，每次都记录图像\n# 确保模型处于评估模式\n","metadata":{"_uuid":"ca2de490-a6d3-442b-ae6f-6ec9c9ed6d70","_cell_guid":"b7ef133e-78ff-4942-b372-18d4909f3531","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 初始化数据模块\ndata_module = MOADataModule(\n    data_dir=aug_data_dir,\n    batch_size=16\n)\n\n# 设置数据模块（准备数据）\ndata_module.setup()\n\ntrainer.validate(\n    model,\n    dataloaders=data_module.val_dataloader(),\n    verbose=True\n)","metadata":{"_uuid":"3b5689d1-ff8c-4930-88fa-c9dfa38f5195","_cell_guid":"5725cdac-948a-4953-8a35-18186f67bdb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-10T09:24:22.951635Z","iopub.execute_input":"2024-04-10T09:24:22.952033Z","iopub.status.idle":"2024-04-10T09:24:25.007678Z","shell.execute_reply.started":"2024-04-10T09:24:22.952003Z","shell.execute_reply":"2024-04-10T09:24:25.006627Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32a661e76c34489a77641a371d5795c"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[{}]"},"metadata":{}}]},{"cell_type":"code","source":"def list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        print(f'{indent}{os.path.basename(root)}/')\n        subindent = ' ' * 4 * (level + 1)\n        for f in files:\n            print(f'{subindent}{f}')\n\n\n# 指定你的目录路径\ndirectory_path = '/kaggle/working/UNet_Compare/'\nlist_files(directory_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:24:27.901104Z","iopub.execute_input":"2024-04-10T09:24:27.901473Z","iopub.status.idle":"2024-04-10T09:24:27.912262Z","shell.execute_reply.started":"2024-04-10T09:24:27.90144Z","shell.execute_reply":"2024-04-10T09:24:27.91088Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"/\nqbzuldwm/\n    checkpoints/\n        epoch=199-step=5200.ckpt\n","output_type":"stream"}]},{"cell_type":"code","source":"# 创建一个新的Artifact，指定其类型为'model'和Artifact的名称\nartifact = wandb.Artifact('model-epo200-sece', type='model')\nartifact.add_file('/kaggle/working/UNet_Compare/qbzuldwm/checkpoints/epoch=199-step=5200.ckpt')\n\n# 保存Artifact到wandb\nwandb.log_artifact(artifact)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:25:58.434513Z","iopub.execute_input":"2024-04-10T09:25:58.434921Z","iopub.status.idle":"2024-04-10T09:26:00.143008Z","shell.execute_reply.started":"2024-04-10T09:25:58.434888Z","shell.execute_reply":"2024-04-10T09:26:00.141798Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<Artifact model-epo200-sece>"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:26:02.613627Z","iopub.execute_input":"2024-04-10T09:26:02.614023Z","iopub.status.idle":"2024-04-10T09:26:07.476141Z","shell.execute_reply.started":"2024-04-10T09:26:02.613987Z","shell.execute_reply":"2024-04-10T09:26:07.475111Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='402.439 MB of 402.439 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25080d111cb94a078646436fcadfb5d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▇▄▆▄▆▄█▂█▂█▂█▂█▂█▂█▁▄▆▄▆▄▄▆▄▆▄▆▄▆▄▆▄▆▄██</td></tr><tr><td>train_loss</td><td>██▆▆▆▅▅▅▅▅▅▄▅▅▅▅▅▄▅▄▅▅▅▄▄▂▂▂▂▁▂▁▂▁▁▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▁▃▃▁▃▃▄▄▁▄▄▂▅▅▅▅▂▆▆▂▆▆▇▇▂▇▇▂███▁</td></tr><tr><td>val_Dice</td><td>▁▄▆▇▇▇▇█▇████████████████▅▇▇█▇█▇███████▇</td></tr><tr><td>val_F1</td><td>▁▁▅▇▇▇▇█▇▇▇█▇██▇▇██████▇█▅█▇█▅█▆█▇█▇███▇</td></tr><tr><td>val_Jaccard</td><td>▁▂▅▇▆▇▇█▇█▇█▇█▇██████████▄█▇█▅█▅█▇█▇███▇</td></tr><tr><td>val_acc_10</td><td>▁▁▁▁▁▁▁▄▃▅▂▅▅▄▆▆▆▅▆▆▅▆▇▆▇▂▃▅▅▂▇▁▆▆▆▇▆█▆▅</td></tr><tr><td>val_acc_12</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▇▇▇▇▇▆▆█▄▃█▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_13</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>val_acc_4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>val_acc_5</td><td>▁▁▁▁▁▁▆█▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▆▇▆▇▆▇▇▇▇▇▇▇█▇▆</td></tr><tr><td>val_accuracy_MACRO</td><td>▁▁▅▅▅▆▆▆▅▆▆▆▇▇▇▇█▇▇██▇█▇█▆▇▇█▆█▇███▇████</td></tr><tr><td>val_accuracy_micro</td><td>▁▁▅▇▇▇▇█▇▇▇█▇██▇▇██████▇█▅█▇█▅█▆█▇█▇███▇</td></tr><tr><td>val_loss</td><td>█▇▆▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▂▁▂▁▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>lr-Adam</td><td>0.00271</td></tr><tr><td>train_loss</td><td>0.13102</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_Dice</td><td>0.95137</td></tr><tr><td>val_F1</td><td>0.94467</td></tr><tr><td>val_Jaccard</td><td>0.92982</td></tr><tr><td>val_acc_10</td><td>0.46875</td></tr><tr><td>val_acc_12</td><td>0.0</td></tr><tr><td>val_acc_13</td><td>0.0</td></tr><tr><td>val_acc_4</td><td>0.0</td></tr><tr><td>val_acc_5</td><td>0.93205</td></tr><tr><td>val_accuracy_MACRO</td><td>0.67632</td></tr><tr><td>val_accuracy_micro</td><td>0.94467</td></tr><tr><td>val_loss</td><td>0.06907</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">test-scec</strong> at: <a href='https://wandb.ai/team-mykcs/UNet_Compare/runs/qbzuldwm' target=\"_blank\">https://wandb.ai/team-mykcs/UNet_Compare/runs/qbzuldwm</a><br/> View project at: <a href='https://wandb.ai/team-mykcs/UNet_Compare' target=\"_blank\">https://wandb.ai/team-mykcs/UNet_Compare</a><br/>Synced 5 W&B file(s), 50 media file(s), 1 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240410_075848-qbzuldwm/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Following Process (Continual Training)","metadata":{}},{"cell_type":"markdown","source":"**if you just finished up training above**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"51fc1ba5-2cb0-4d6c-9869-a2818e01d7a2","_cell_guid":"ddb49941-070c-4aba-8d7b-d77746b33eea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**if you want to continual training from wandb**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(\n    project=\"UNet_Compare\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nart = wandb.Artifact('model-epo120-04100227', type='model')\n# ... add content to artifact ...\nartifact.add_file('/kaggle/working/UNet_Compare/3huy70i6/checkpoints/epoch=119-step=3960.ckpt')\n\nwandb.log_artifact(art)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义数据增强\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.2),\n    A.VerticalFlip(p=0.2),\n    A.RandomRotate90(),\n    #     A.RandomBrightnessContrast(p=0.2),\n    A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.3, alpha_affine=120 * 0.2),\n    A.RandomSizedCrop(min_max_height=(128, 256), height=256, width=256, p=0.5),\n    # A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    # A.Normalize(mean=(0.5,), std=(0.5,)),\n    ToTensorV2(),\n])\n\n\ndef adjust_window(image, window_center, window_width):\n    \"\"\"\n    调整CT图像的窗宽窗位。\n    :param image: 输入的图像数组。\n    :param window_center: 窗位（WC）。\n    :param window_width: 窗宽（WW）。\n    :return: 调整窗宽窗位后的图像。\n    \"\"\"\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    windowed_img = np.clip(image, img_min, img_max)\n    # print(windowed_img.dtype) # NOW its float64\n    return windowed_img\n\n\nclass MultipleImageDataset(Dataset):\n    def __init__(self, image_paths, label_paths, transform=None):\n        \"\"\"\n        image_paths: 图像文件路径列表\n        label_paths: 标签文件路径列表\n        transform: 应用于图像和标签的转换操作\n        \"\"\"\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n        self.transform = transform\n\n    def __len__(self):\n        # 假设图像和标签列表长度相等\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):  # dataloader获取每个数据都会用到这个函数，所以你应当在这里实现你需要的\n        # 预处理等等步骤\n        image = (np.load(self.image_paths[idx]))['arr_0']\n        #         print(image.shape)\n        label = (np.load(self.label_paths[idx]))['arr_0']\n        #         print(label.shape)\n\n        image = adjust_window(image, window_center=40, window_width=400)\n\n        if self.transform:\n            #             image = image.astype(np.float32)\n            augmented = self.transform(image=image, mask=label)\n            image = augmented['image']\n            #             print(\"image aug\")\n            label = augmented['mask']\n            image = image.float()\n            label = label.long()\n\n        label = label.long()\n        image = (image - image.min()) / (image.max() - image.min())\n        return image.float(), label.long()\n\n\n######################################################################################################################\nclass MOADataModule(LightningDataModule):\n    def __init__(self, data_dir: str, batch_size: int = 16):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.train_transform = train_transform\n        self.val_transform = val_transform\n\n    def setup(self, stage=None):\n        image_dir = os.path.join(self.data_dir, 'image_npz')  # 注意这里路径的更正\n        label_dir = os.path.join(self.data_dir, 'mask_npz')\n\n        # 读取文件路径\n        image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.npz')])\n        label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.npz')])\n\n        # 划分训练集、验证集、测试集\n        train_size = int(0.8 * len(image_files))\n        val_size = int(0.1 * len(image_files))\n\n        self.train_image_paths = image_files[:train_size]\n        self.val_image_paths = image_files[train_size:train_size + val_size]\n        self.test_image_paths = image_files[train_size + val_size:]\n\n        self.train_label_paths = label_files[:train_size]\n        self.val_label_paths = label_files[train_size:train_size + val_size]\n        self.test_label_paths = label_files[train_size + val_size:]\n\n    def train_dataloader(self):\n        train_dataset = MultipleImageDataset(self.train_image_paths, self.train_label_paths,\n                                             transform=self.train_transform)\n        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n\n    def val_dataloader(self):\n        val_dataset = MultipleImageDataset(self.val_image_paths, self.val_label_paths, transform=self.val_transform)\n        return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    def test_dataloader(self):\n        test_dataset = MultipleImageDataset(self.test_image_paths, self.test_label_paths, transform=self.val_transform)\n        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n\n# 定义数据集和数据加载器\n# WARNING: 下面的数据目录是针对特定Kaggle数据集的示例路径。\n#          请根据您的实际数据集位置修改此路径。\ndata_dir = '/kaggle/input/aug-dataset-for-fine-tune/AUG_dataset'\naug_data_dir = data_dir\n\ndef draw_bounding_boxes(ax, mask, class_ids):\n    \"\"\" 在给定的轴上绘制边界框，突出显示特定类别 \"\"\"\n    for class_id in class_ids:\n        positions = np.argwhere(mask == class_id)\n        if positions.size > 0:\n            xmin, xmax = positions[:, 0].min(), positions[:, 0].max()\n            ymin, ymax = positions[:, 1].min(), positions[:, 1].max()\n            rect = Rectangle((ymin, xmin), ymax - ymin, xmax - xmin, linewidth=2, edgecolor='red', facecolor='none')\n            ax.add_patch(rect)\n            ax.text(ymin, xmin, f'Class {class_id}', color='red', fontsize=12, va='top', ha='left')\n\n\ndef predict_and_log_images(num_samples=2, model=model_ckpt, data_module=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    test_loader = data_module.test_dataloader()\n\n    # 生成随机索引\n    indices = torch.randperm(len(test_loader.dataset))[:num_samples]\n    special_classes = [4, 5, 10, 12, 13]\n\n    fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n    cmap = plt.get_cmap('tab20')\n    for i, idx in enumerate(indices):\n        image, mask = test_loader.dataset[idx]\n        class_labels = np.unique(mask)\n        colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n        image = image.unsqueeze(0).to(device)\n        mask = mask.squeeze()\n\n        with torch.no_grad():\n            pred = model(image)\n            prediction = torch.argmax(pred, dim=1).cpu().squeeze()\n\n        axs[i, 0].imshow(image.squeeze().cpu().numpy(), cmap='gray')\n        axs[i, 0].set_title(f'Original Image {i + 1}')\n        axs[i, 0].axis('off')\n\n        axs[i, 1].imshow(mask.cpu().numpy(), cmap='tab20')\n        axs[i, 1].set_title(f'True Mask {i + 1}')\n        axs[i, 1].axis('off')\n\n        axs[i, 2].imshow(prediction.numpy(), cmap='tab20')\n        axs[i, 2].set_title(f'Predicted Mask {i + 1}')\n        axs[i, 2].axis('off')\n\n        # 在真实和预测掩码上绘制边界框\n        draw_bounding_boxes(axs[i, 1], mask.cpu().numpy(), special_classes)\n        draw_bounding_boxes(axs[i, 2], prediction.numpy(), special_classes)\n\n    legend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\n    fig.legend(handles=legend_elements, loc='upper center', ncol=len(class_labels), title=\"Classes\")\n    plt.tight_layout()\n    plt.close(fig)  # 防止在notebook中显示图像\n\n    return fig\n\n\nclass ValidationCallback(Callback):\n    def on_validation_epoch_end(self, trainer, pl_module):\n        # 每10个epoch执行一次\n        if (trainer.current_epoch + 1) % 1 == 0:\n            fig = predict_and_log_images(num_samples=2, model=model_ckpt, data_module=data_module)\n            # 用wandb记录图像，或进行其他操作\n            wandb.log({\"Validation Callback Predicted Images\": wandb.Image(fig)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  初始化数据模块\ndata_module = MOADataModule(data_dir=aug_data_dir, batch_size=16)\n# 设置数据模块（准备数据）\ndata_module.setup()\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ckpt.to(device)\nmodel_ckpt.eval()  # 设置为评估模式\n\n# 获取测试数据\ntest_loader = data_module.test_dataloader()\nspecial_classes = [4, 5, 10, 12, 13]\n# 取一个批次的数据进行演示\nfor images, masks in test_loader:\n    images, masks = images.to(device), masks.to(device)\n    with torch.no_grad():\n        predictions = model_ckpt(images)  # 进行预测\n    predictions = torch.argmax(predictions, dim=1)  # 转换成类别标签\n    break  # 这里我们只处理一个批次作为示例\n\n# 将数据转移到CPU并转换为numpy\nimages = images.cpu().numpy()\nmasks = masks.cpu().numpy()\npredictions = predictions.cpu().numpy()\n\n# 显示图像、真实掩码和预测掩码\nfig, axs = plt.subplots(len(images), 3, figsize=(20, 5 * len(images)))  # 根据批次大小设置子图\nclass_labels = np.unique(masks)  # 获取类别标签\ncmap = plt.get_cmap('tab20')  # 获取颜色映射\ncolors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n\nfor i, (img, mask, pred) in enumerate(zip(images, masks, predictions)):\n    axs[i, 0].imshow(img[0], cmap='gray')  # 假设图片是单通道的\n    axs[i, 0].set_title('Original Image')\n    axs[i, 0].axis('off')\n\n    axs[i, 1].imshow(mask, cmap=cmap)  # 假设掩码是单通道的\n    axs[i, 1].set_title('True Mask')\n    axs[i, 1].axis('off')\n\n    axs[i, 2].imshow(pred, cmap=cmap)\n    axs[i, 2].set_title('Predicted Mask')\n    axs[i, 2].axis('off')\n\n    draw_bounding_boxes(axs[i, 1], mask, special_classes)\n    draw_bounding_boxes(axs[i, 2], mask, special_classes)\n\n# 创建图例\nlegend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 1), ncol=len(class_labels),\n           title=\"Classes\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])  # 调整子图布局，留出底部空间给图例\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb_logger = WandbLogger(project=\"SMU MOA\", name=\"ResUNetPP50_monaiDiceCELoss_Max150\")\nwandb_logger = WandbLogger(\n    project=\"UNet_Compare\",\n    name=\"Cont-ResUNet_epo120_DiceCELosswithKL-AugData\"\n)\n\n# 设置ModelCheckpoint以每20轮保存一次模型\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='my_model/',  # 模型保存路径\n    filename='model-{epoch:02d}',  # 文件名包含 epoch\n    save_top_k=-1,  # 设置为 -1 以保存所有检查点\n    every_n_epochs=20,  # 每20轮保存一次\n    save_on_train_epoch_end=True  # 确保在训练轮结束时保存\n)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_monitor = LearningRateMonitor(logging_interval='step')\n# 假设你已经定义了 LiTSDataModule\ndata_module = MOADataModule(data_dir=data_dir, batch_size=16)\n# 初始化模型和训练器\nmodel = UNetTestModel()\n\ntrainer = Trainer(\n    max_epochs=120,\n    # fast_dev_run=True, \n    logger=wandb_logger,\n    callbacks=[lr_monitor, checkpoint_callback, ValidationCallback()],\n    # callbacks=[lr_monitor],\n    log_every_n_steps=1,\n    check_val_every_n_epoch=1,\n    # precision='16-mixed',\n)\n\n# 使用更新后的学习率继续训练\ntrainer.fit(model, datamodule=data_module)\n\n#  初始化数据模块\ndata_module = MOADataModule(\n    data_dir=aug_data_dir,\n    batch_size=16\n)\n# 设置数据模块（准备数据）\ndata_module.setup()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ckpt.to(device)\nmodel_ckpt.eval()  # 设置为评估模式\n\n# 获取测试数据\ntest_loader = data_module.test_dataloader()\nspecial_classes = [4, 5, 10, 12, 13]\n# 取一个批次的数据进行演示\nfor images, masks in test_loader:\n    images, masks = images.to(device), masks.to(device)\n    with torch.no_grad():\n        predictions = model_ckpt(images)  # 进行预测\n    predictions = torch.argmax(predictions, dim=1)  # 转换成类别标签\n    break  # 这里我们只处理一个批次作为示例\n\n# 将数据转移到CPU并转换为numpy\nimages = images.cpu().numpy()\nmasks = masks.cpu().numpy()\npredictions = predictions.cpu().numpy()\n\n# 显示图像、真实掩码和预测掩码\nfig, axs = plt.subplots(len(images), 3, figsize=(20, 5 * len(images)))  # 根据批次大小设置子图\nclass_labels = np.unique(masks)  # 获取类别标签\ncmap = plt.get_cmap('tab20')  # 获取颜色映射\ncolors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n\nfor i, (img, mask, pred) in enumerate(zip(images, masks, predictions)):\n    axs[i, 0].imshow(img[0], cmap='gray')  # 假设图片是单通道的\n    axs[i, 0].set_title('Original Image')\n    axs[i, 0].axis('off')\n\n    axs[i, 1].imshow(mask, cmap=cmap)  # 假设掩码是单通道的\n    axs[i, 1].set_title('True Mask')\n    axs[i, 1].axis('off')\n\n    axs[i, 2].imshow(pred, cmap=cmap)\n    axs[i, 2].set_title('Predicted Mask')\n    axs[i, 2].axis('off')\n\n    draw_bounding_boxes(axs[i, 1], mask, special_classes)\n    draw_bounding_boxes(axs[i, 2], mask, special_classes)\n\n# 创建图例\nlegend_elements = [Patch(facecolor=colors[i], label=f'Class {class_labels[i]}') for i in range(len(class_labels))]\nfig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 1), ncol=len(class_labels),\n           title=\"Classes\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])  # 调整子图布局，留出底部空间给图例\nplt.show()\nwandb.log({\"Predicted Images After Continual Train\": wandb.Image(fig)})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}